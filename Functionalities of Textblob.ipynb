{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What is Textblob\n",
    "\n",
    "* Textblob is a Python library for processing textual data. It is used to perform natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis Lemmatization, Stemming, Tokenization, and N-Grams.\n",
    "* It is faster than NLTK, however it does not provide the functionalities like vectorization and dependency parsing.\n",
    "* Official Link to Textblob: https://textblob.readthedocs.io/en/dev/\n",
    "* How to Install Textblob: pip install textblob\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T19:51:36.879003Z",
     "start_time": "2025-09-12T19:51:34.483790Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --upgrade pip",
   "id": "b6c416fdbbef3cfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (24.3.1)\r\n",
      "Collecting pip\r\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\r\n",
      "Installing collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.3.1\r\n",
      "    Uninstalling pip-24.3.1:\r\n",
      "      Successfully uninstalled pip-24.3.1\r\n",
      "Successfully installed pip-25.2\r\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T19:51:42.237596Z",
     "start_time": "2025-09-12T19:51:40.113408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install Textblob\n",
    "!pip install nltk\n",
    "!pip install textblob"
   ],
   "id": "5fde4e1e5caf191e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk) (1.5.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk) (2025.9.1)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from nltk) (4.67.1)\r\n",
      "Requirement already satisfied: textblob in ./.venv/lib/python3.11/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: nltk>=3.9 in ./.venv/lib/python3.11/site-packages (from textblob) (3.9.1)\r\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk>=3.9->textblob) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk>=3.9->textblob) (1.5.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>=3.9->textblob) (2025.9.1)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from nltk>=3.9->textblob) (4.67.1)\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:05:50.926226Z",
     "start_time": "2025-09-12T20:05:50.913719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('popular') # Fetches commonly used NLTK datasets/models"
   ],
   "id": "f162deeb1fec1124",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:06.744317Z",
     "start_time": "2025-09-12T20:32:06.737955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger') # This helps with Tagging tasks."
   ],
   "id": "18d63be113227e0a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Functionalities of Textblob\n",
    "\n",
    "* Language Detection\n",
    "* Word Correction\n",
    "* Word Count\n",
    "* Phrase Extraction\n",
    "* POS Tagging\n",
    "* Tokenization\n",
    "* Pluralization of words using Textblob\n",
    "* Lemmatization using Textblob\n",
    "* N-Grams in Textblob"
   ],
   "id": "ed73f2ed69da5cf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Language Detection\n",
    "\n",
    "* Language detection is a process of identifying the language of a given text.\n",
    "* Textblob uses the langdetect library to detect the language of a given text.\n",
    "* Textblob is also able to translate text from one language to another language.\n",
    "* langdetect returns ISO 639-1 codes like \"en\", \"es\", etc.\n",
    "* deep-translator uses public endpoints but can occasionally be rate-limited. For production/reliable use, prefer the official Google Cloud Translate API."
   ],
   "id": "3a08b1cd4c2f971d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T19:51:54.622486Z",
     "start_time": "2025-09-12T19:51:53.456819Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install langdetect deep-translator",
   "id": "a83beea88464a425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in ./.venv/lib/python3.11/site-packages (1.0.9)\r\n",
      "Requirement already satisfied: deep-translator in ./.venv/lib/python3.11/site-packages (1.11.4)\r\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.11/site-packages (from langdetect) (1.17.0)\r\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in ./.venv/lib/python3.11/site-packages (from deep-translator) (4.13.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in ./.venv/lib/python3.11/site-packages (from deep-translator) (2.32.5)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.8.3)\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T19:53:19.185626Z",
     "start_time": "2025-09-12T19:53:17.950946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "text = \"Hello Jeffrey, how are you?\"\n",
    "\n",
    "# Detect language\n",
    "detected_lang = detect(text)\n",
    "print(\"Detected Language is:\", detected_lang)\n",
    "\n",
    "# Translate to Spanish\n",
    "translated = GoogleTranslator(source='auto', target='es').translate(text)\n",
    "print(\"Input text in Spanish:\", translated)"
   ],
   "id": "dbe10ee46f7c0338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language is: en\n",
      "Input text in Spanish: Hola Jeffrey, ¿cómo estás?\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Spelling Correction",
   "id": "97dd2926dba85dd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:05:49.416921Z",
     "start_time": "2025-09-12T20:05:49.411349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from textblob import TextBlob\n",
    "text = \"\"\"ABCD Corporation alays values ttheir employees!!!\"\"\""
   ],
   "id": "f2880ae19bb4b362",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:05:52.817883Z",
     "start_time": "2025-09-12T20:05:52.813792Z"
    }
   },
   "cell_type": "code",
   "source": "print(text)",
   "id": "995d384652716b10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD Corporation alays values ttheir employees!!!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:05:58.527884Z",
     "start_time": "2025-09-12T20:05:58.525621Z"
    }
   },
   "cell_type": "code",
   "source": "blob = TextBlob(text)",
   "id": "35de320e6c300794",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:06:11.005226Z",
     "start_time": "2025-09-12T20:06:10.946195Z"
    }
   },
   "cell_type": "code",
   "source": "blob.correct()",
   "id": "aea1a4897fd5efed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"ABCD Corporation always values their employees!!!\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:06:37.652266Z",
     "start_time": "2025-09-12T20:06:37.574115Z"
    }
   },
   "cell_type": "code",
   "source": "TextBlob('hasss').correct()",
   "id": "98b3b64c78aae8e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"has\")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:07:25.943708Z",
     "start_time": "2025-09-12T20:07:25.933469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Notice, that sometimes it fails\n",
    "TextBlob(\"ur food is great\").correct() # \"ur\" should be \"your\""
   ],
   "id": "8c6913b65780ecc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"or food is great\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word Count\n",
    "\n",
    "With the help of word count, we can count the frequency of words or a noun phrase in a given sentence."
   ],
   "id": "ead8815b6ce6a23a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:02.749219Z",
     "start_time": "2025-09-12T20:32:00.076377Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m textblob.download_corpora",
   "id": "bd3de70a720fe5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\r\n",
      "[nltk_data] Downloading package punkt_tab to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\r\n",
      "[nltk_data] Downloading package wordnet to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Package wordnet is already up-to-date!\r\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\r\n",
      "[nltk_data] Downloading package conll2000 to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\r\n",
      "[nltk_data] Downloading package movie_reviews to\r\n",
      "[nltk_data]     /Users/jeffreyjackson/nltk_data...\r\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:05.497199Z",
     "start_time": "2025-09-12T20:32:05.493686Z"
    }
   },
   "cell_type": "code",
   "source": "text = \"Sentiment Analysis is a process by which we can find the sentiment of a text. Sentiment can be Positive, Negative or Neutral.\"",
   "id": "d1c5a99cc215620",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:08.402867Z",
     "start_time": "2025-09-12T20:32:08.399959Z"
    }
   },
   "cell_type": "code",
   "source": "blob  = TextBlob(text)",
   "id": "b190a98e84d80652",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:14.210769Z",
     "start_time": "2025-09-12T20:32:14.181450Z"
    }
   },
   "cell_type": "code",
   "source": "blob.word_counts[\"analysis\"]",
   "id": "c656c419678063df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:32:53.605559Z",
     "start_time": "2025-09-12T20:32:53.596297Z"
    }
   },
   "cell_type": "code",
   "source": "blob.word_counts[\"Sentiment\"]",
   "id": "194367f68631e8ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:33:02.886171Z",
     "start_time": "2025-09-12T20:33:02.879073Z"
    }
   },
   "cell_type": "code",
   "source": "blob.word_counts[\"sentiment\"]",
   "id": "59f8049f0112770e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:33:11.299435Z",
     "start_time": "2025-09-12T20:33:11.293390Z"
    }
   },
   "cell_type": "code",
   "source": "blob.word_counts[\"Analysis\"]",
   "id": "7459e0f5e0574241",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOTE: When counting words, capitalization is ignored and the word is converted to lowercase.",
   "id": "be3c6268c06a86e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### POS Tagging (Part-of-Speech)",
   "id": "c0259b1352f49490"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:38:59.281160Z",
     "start_time": "2025-09-12T20:38:59.062791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(\"My name is Jeff. I like to read about NLP. I work at ABCD Company.\")\n",
    "print(text.tags)"
   ],
   "id": "aeee9da25598af8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Jeff', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('NLP', 'NNP'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('ABCD', 'NNP'), ('Company', 'NNP')]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:43:51.985705Z",
     "start_time": "2025-09-12T20:43:51.971413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_tuple=[]\n",
    "for i in text.tags:\n",
    "    print(i)\n",
    "    if 'VBP' not in i[1]:\n",
    "        new_tuple.append(i)"
   ],
   "id": "f6966ef7d0d2e424",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My', 'PRP$')\n",
      "('name', 'NN')\n",
      "('is', 'VBZ')\n",
      "('Jeff', 'NNP')\n",
      "('I', 'PRP')\n",
      "('like', 'VBP')\n",
      "('to', 'TO')\n",
      "('read', 'VB')\n",
      "('about', 'IN')\n",
      "('NLP', 'NNP')\n",
      "('I', 'PRP')\n",
      "('work', 'VBP')\n",
      "('at', 'IN')\n",
      "('ABCD', 'NNP')\n",
      "('Company', 'NNP')\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:43:54.847702Z",
     "start_time": "2025-09-12T20:43:54.838910Z"
    }
   },
   "cell_type": "code",
   "source": "new_tuple",
   "id": "4c2413f9ec0e863e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Jeff', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('read', 'VB'),\n",
       " ('about', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('at', 'IN'),\n",
       " ('ABCD', 'NNP'),\n",
       " ('Company', 'NNP')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:44:44.250675Z",
     "start_time": "2025-09-12T20:44:44.247523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "value=''\n",
    "for i in new_tuple:\n",
    "    value = value +\" \" + \"\".join(i[0])"
   ],
   "id": "b62f24521108d232",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:44:50.898283Z",
     "start_time": "2025-09-12T20:44:50.893342Z"
    }
   },
   "cell_type": "code",
   "source": "value",
   "id": "15665bd4c980d0c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Jeff I to read about NLP I at ABCD Company'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tokenization\n",
    "* Corpus (or corpora in plural) is a collection of texts or documents.\n",
    "* Tokenization is the process of breaking down a corpus into smaller units, called tokens.\n",
    "* Tokens are the basic units of text, such as words, characters, or subwords.\n",
    "* Tokens are the total number of words in a text (corpus), regardless of their frequency or occurrence in the text. Tokens are a string of consecutive characters that lies between two spaces or a space and punctuation.\n",
    "* For example, the corpus \"Hello Jeffrey, how are you?\" has 6 tokens: \"Hello\", \"Jeffrey\", \"how\", \"are\", \"you\", and \"?\".\n",
    "* Another example, if you ahve a string \"abc_123_defg\", and you split it based on the underscore value, it will be tokenized as \"abc\", \"123\", and \"defg\"."
   ],
   "id": "4dce31938af3d757"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:54:53.327108Z",
     "start_time": "2025-09-15T20:54:53.223141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "R is a comprehensive statistical and graphical programming language, which is growing in popularity among data analysts.\"\"\""
   ],
   "id": "955c8a7d63bba8af",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:54:55.055362Z",
     "start_time": "2025-09-15T20:54:55.039565Z"
    }
   },
   "cell_type": "code",
   "source": "blob_object = TextBlob(text)",
   "id": "3de0379b55f2d126",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:55:05.357724Z",
     "start_time": "2025-09-15T20:55:05.322164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word tokenization of the sample corpus\n",
    "corpus_words = blob_object.words"
   ],
   "id": "99c6afc859e000d7",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:55:07.933118Z",
     "start_time": "2025-09-15T20:55:07.918546Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_words",
   "id": "e2839c19c06dd058",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['R', 'is', 'a', 'comprehensive', 'statistical', 'and', 'graphical', 'programming', 'language', 'which', 'is', 'growing', 'in', 'popularity', 'among', 'data', 'analysts'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:55:27.016430Z",
     "start_time": "2025-09-15T20:55:26.994203Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(corpus_words))",
   "id": "af4ddd3e221db458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:56:14.916681Z",
     "start_time": "2025-09-15T20:56:14.900686Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_sentences = blob_object.sentences",
   "id": "251e740044e9efb5",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:56:16.376070Z",
     "start_time": "2025-09-15T20:56:16.350676Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_sentences",
   "id": "7d3fc5527d4fa9dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " R is a comprehensive statistical and graphical programming language, which is growing in popularity among data analysts.\")]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:56:21.701513Z",
     "start_time": "2025-09-15T20:56:21.694440Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(corpus_sentences))",
   "id": "9fee8459c03daf09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pluralization of words using Textblob",
   "id": "f0d7cef04f183d2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:59:53.087849Z",
     "start_time": "2025-09-15T20:59:53.046185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ],
   "id": "b10c2876072fa9e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T20:59:58.303120Z",
     "start_time": "2025-09-15T20:59:58.287948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from textblob import Word\n",
    "w = Word('Platforms')\n",
    "w.pluralize()"
   ],
   "id": "f1f052ce1dd02e01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platformss'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:02:21.384554Z",
     "start_time": "2025-09-15T21:02:21.358453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science.  \\n It helps the community through blogs, YouTube, GLA, etc\")\n",
    "for word, pos in blob.tags:\n",
    "    if pos == 'NN':\n",
    "        print (word.pluralize())"
   ],
   "id": "ae89c403f1cd93f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platforms\n",
      "sciences\n",
      "communities\n",
      "etcs\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lemmatization using Textblob",
   "id": "494a5439630aae6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:06:05.326686Z",
     "start_time": "2025-09-15T21:06:02.600800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science.  \\n It helps the community through blogs, YouTube, GLA, etc\")\n",
    "words = blob.words\n",
    "\n",
    "for word in words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ],
   "id": "a8e317762a756ebf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Great | LEMMA: Great | STEM: great\n",
      "ORIGINAL: Learning | LEMMA: Learning | STEM: learn\n",
      "ORIGINAL: is | LEMMA: is | STEM: is\n",
      "ORIGINAL: a | LEMMA: a | STEM: a\n",
      "ORIGINAL: great | LEMMA: great | STEM: great\n",
      "ORIGINAL: platform | LEMMA: platform | STEM: platform\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: learn | LEMMA: learn | STEM: learn\n",
      "ORIGINAL: data | LEMMA: data | STEM: data\n",
      "ORIGINAL: science | LEMMA: science | STEM: scienc\n",
      "ORIGINAL: It | LEMMA: It | STEM: it\n",
      "ORIGINAL: helps | LEMMA: help | STEM: help\n",
      "ORIGINAL: the | LEMMA: the | STEM: the\n",
      "ORIGINAL: community | LEMMA: community | STEM: commun\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: blogs | LEMMA: blog | STEM: blog\n",
      "ORIGINAL: YouTube | LEMMA: YouTube | STEM: youtub\n",
      "ORIGINAL: GLA | LEMMA: GLA | STEM: gla\n",
      "ORIGINAL: etc | LEMMA: etc | STEM: etc\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:06:46.450118Z",
     "start_time": "2025-09-15T21:06:46.440084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = Word(\"learning\")\n",
    "w.lemmatize(\"n\") # n for noun"
   ],
   "id": "fac27654c575f9f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:07:02.118648Z",
     "start_time": "2025-09-15T21:07:02.104843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = Word(\"learning\")\n",
    "w.lemmatize(\"v\") # v for verb"
   ],
   "id": "7289b2d15c9db45d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:07:13.189603Z",
     "start_time": "2025-09-15T21:07:13.181085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = Word(\"peoples\")\n",
    "w.lemmatize(\"n\") # n for noun"
   ],
   "id": "abf05286bce8d246",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "n-gram in Textblob\n",
    "\n",
    "An N-gram is an N-token sequence of words. A 2gram (more commonly call a bigram) is a two-word sequence of words like \"really good\", \"not good\", or \"your homework\". A 3-gram (more commonly called a trigram) is a three-word sequence of words like \"not at all\", or \"I am happy\"."
   ],
   "id": "27b9bfeca6ef0cbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:17:28.547417Z",
     "start_time": "2025-09-15T21:17:28.520213Z"
    }
   },
   "cell_type": "code",
   "source": "blob",
   "id": "f70300f3aff43e82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Great Learning is a great platform to learn data science.  \n",
       " It helps the community through blogs, YouTube, GLA, etc\")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:17:34.139537Z",
     "start_time": "2025-09-15T21:17:34.131093Z"
    }
   },
   "cell_type": "code",
   "source": "blob.ngrams(n=1)",
   "id": "871243aee4b2f2cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great']),\n",
       " WordList(['Learning']),\n",
       " WordList(['is']),\n",
       " WordList(['a']),\n",
       " WordList(['great']),\n",
       " WordList(['platform']),\n",
       " WordList(['to']),\n",
       " WordList(['learn']),\n",
       " WordList(['data']),\n",
       " WordList(['science']),\n",
       " WordList(['It']),\n",
       " WordList(['helps']),\n",
       " WordList(['the']),\n",
       " WordList(['community']),\n",
       " WordList(['through']),\n",
       " WordList(['blogs']),\n",
       " WordList(['YouTube']),\n",
       " WordList(['GLA']),\n",
       " WordList(['etc'])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:17:38.765542Z",
     "start_time": "2025-09-15T21:17:38.759057Z"
    }
   },
   "cell_type": "code",
   "source": "blob.ngrams(n=2)",
   "id": "802f0c7fa560725",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning']),\n",
       " WordList(['Learning', 'is']),\n",
       " WordList(['is', 'a']),\n",
       " WordList(['a', 'great']),\n",
       " WordList(['great', 'platform']),\n",
       " WordList(['platform', 'to']),\n",
       " WordList(['to', 'learn']),\n",
       " WordList(['learn', 'data']),\n",
       " WordList(['data', 'science']),\n",
       " WordList(['science', 'It']),\n",
       " WordList(['It', 'helps']),\n",
       " WordList(['helps', 'the']),\n",
       " WordList(['the', 'community']),\n",
       " WordList(['community', 'through']),\n",
       " WordList(['through', 'blogs']),\n",
       " WordList(['blogs', 'YouTube']),\n",
       " WordList(['YouTube', 'GLA']),\n",
       " WordList(['GLA', 'etc'])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:17:43.348376Z",
     "start_time": "2025-09-15T21:17:43.337002Z"
    }
   },
   "cell_type": "code",
   "source": "blob.ngrams(n=3)",
   "id": "82b0cce61080c38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is']),\n",
       " WordList(['Learning', 'is', 'a']),\n",
       " WordList(['is', 'a', 'great']),\n",
       " WordList(['a', 'great', 'platform']),\n",
       " WordList(['great', 'platform', 'to']),\n",
       " WordList(['platform', 'to', 'learn']),\n",
       " WordList(['to', 'learn', 'data']),\n",
       " WordList(['learn', 'data', 'science']),\n",
       " WordList(['data', 'science', 'It']),\n",
       " WordList(['science', 'It', 'helps']),\n",
       " WordList(['It', 'helps', 'the']),\n",
       " WordList(['helps', 'the', 'community']),\n",
       " WordList(['the', 'community', 'through']),\n",
       " WordList(['community', 'through', 'blogs']),\n",
       " WordList(['through', 'blogs', 'YouTube']),\n",
       " WordList(['blogs', 'YouTube', 'GLA']),\n",
       " WordList(['YouTube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T21:17:46.961726Z",
     "start_time": "2025-09-15T21:17:46.953637Z"
    }
   },
   "cell_type": "code",
   "source": "blob.ngrams(n=4)",
   "id": "e498a519d2f4659a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is', 'a']),\n",
       " WordList(['Learning', 'is', 'a', 'great']),\n",
       " WordList(['is', 'a', 'great', 'platform']),\n",
       " WordList(['a', 'great', 'platform', 'to']),\n",
       " WordList(['great', 'platform', 'to', 'learn']),\n",
       " WordList(['platform', 'to', 'learn', 'data']),\n",
       " WordList(['to', 'learn', 'data', 'science']),\n",
       " WordList(['learn', 'data', 'science', 'It']),\n",
       " WordList(['data', 'science', 'It', 'helps']),\n",
       " WordList(['science', 'It', 'helps', 'the']),\n",
       " WordList(['It', 'helps', 'the', 'community']),\n",
       " WordList(['helps', 'the', 'community', 'through']),\n",
       " WordList(['the', 'community', 'through', 'blogs']),\n",
       " WordList(['community', 'through', 'blogs', 'YouTube']),\n",
       " WordList(['through', 'blogs', 'YouTube', 'GLA']),\n",
       " WordList(['blogs', 'YouTube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
